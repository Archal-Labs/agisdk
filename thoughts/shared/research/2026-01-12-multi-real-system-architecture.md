---
date: 2026-01-12T21:15:00-08:00
researcher: Claude
git_commit: 05435f7f951e590a68fd6929fc557e5504643e82
branch: main
repository: Archal-Labs/agisdk
topic: "Multi-REAL Benchmark System Architecture and Task Completion Workflow"
tags: [research, codebase, multi-real, benchmark, jmespath, evaluation]
status: complete
last_updated: 2026-01-12
last_updated_by: Claude
---

# Research: Multi-REAL Benchmark System Architecture

**Date**: 2026-01-12T21:15:00-08:00
**Researcher**: Claude
**Git Commit**: 05435f7f951e590a68fd6929fc557e5504643e82
**Branch**: main
**Repository**: Archal-Labs/agisdk

## Research Question
Understand the multi-real/ directory structure, task JSON completion workflow, JMESPath validation status, and the best approach for completing TODO placeholders in expected_value fields.

## Summary

The `multi-real/` directory contains a benchmark system for evaluating AI agents on multi-app browser tasks. The system has:
- **84 task JSON files** in `multi-real/tasks/` with ~260 TODO placeholders remaining
- **JMESPath queries** are generated by Gemini 2.5 Flash via `tools/gen_tasks.py`
- **Validation scripts** exist: `evaluation/validate_queries.py` and `evaluation/validate_ground_truth.py`
- **Ground truth** is captured either manually or synthetically via `capture_pipeline.py` and `tools/gen_ground_truth.py`
- **7 manual ground truth files** exist in `final_states/manual/`

The current gap: Task JSON files have JMESPath queries but `expected_value` fields contain "TODO: Add expected value after checking /finish JSON". The workflow to resolve this involves running validation against ground truth finish states.

## Detailed Findings

### Directory Structure

```
multi-real/
├── tasks/                    # 84 task JSON files
├── tasks.csv                 # Source task definitions (prompts, websites)
├── final_states/
│   ├── manual/              # 7 manually captured ground truth files
│   ├── automated/           # Empty
│   └── synthetic/           # Empty (generated via gen_ground_truth.py)
├── core/
│   ├── registry.py          # MultiRealRegistry - loads tasks from tasks/
│   ├── harness.py           # MultiRealHarness - runs benchmark
│   └── validator.py         # HybridValidator - JMESPath + LLM judge
├── evaluation/
│   ├── validate_queries.py      # Tests JMESPath against raw finish JSON
│   ├── validate_ground_truth.py # Tests JMESPath with expected_value matching
│   ├── llm_judge.py             # LLM-based evaluation
│   └── compare_finishes.py      # Compare finish states
├── tools/
│   ├── gen_tasks.py         # Generates task JSONs from tasks.csv (uses Gemini)
│   ├── gen_ground_truth.py  # Runs model to generate synthetic ground truth
│   ├── sync_tasks.py        # Syncs tasks to package directory
│   └── re_evaluate.py       # Re-evaluates results after query fixes
├── run_benchmark.py         # Main benchmark runner
├── run_suite.py             # Suite runner
├── capture_pipeline.py      # Captures finish states using agent runs
└── test_infrastructure.py   # Infrastructure tests
```

### Task JSON Structure

Each task file (e.g., `dashdish-gomail-1.json`) contains:

```json
{
  "id": "dashdish-gomail-1",
  "goal": "Figure out how much it would cost...",
  "websites": [
    {"id": "dashdish", "name": "DashDish", "similarTo": "DoorDash", "url": "https://real-dashdish.vercel.app"},
    {"id": "gomail", "name": "GoMail", "similarTo": "Gmail", "url": "https://real-gomail.vercel.app"}
  ],
  "difficulty": "hard",
  "challengeType": "action",
  "possible": true,
  "evals": [
    {
      "description": "Verify that items were added to the Dashdish cart...",
      "type": "jmespath",
      "query": "dashdish.initialfinaldiff.added.cart != null",
      "expected_value": "TODO: Add expected value after checking /finish JSON"
    }
  ],
  "points": 2,
  "config": {}
}
```

### The TODO Problem

**Current state:**
- 84 task files with ~260 total TODO placeholders
- Tasks with 0 TODOs (completed): 8 files
- Tasks with 2-5 TODOs each: 76 files

**What the TODOs represent:**
The `expected_value` field should contain the expected result when the JMESPath query runs against a ground truth finish state:
- Boolean queries (e.g., `contains(...)`, `length(...) >= \`1\``) should have `true` or `false`
- Field access queries that return actual values should have `null` (indicating manual verification needed)

### Ground Truth System

**Manual ground truth** (`final_states/manual/`): 7 files
- `dashdish-gomail-1.json` (7MB)
- `flyunified-gocalendar-staynb-1.json` (2.7MB)
- `gomail-marrsuite-1.json` (4.9MB)
- `gomail-omnizon-1.json` (4.9MB)
- `gomail-topwork-3.json` (349KB)
- `gomail-zilloft-3.json` (15MB)
- `opendining-udriver-1.json` (381KB)

**Synthetic ground truth** (`final_states/synthetic/`): Empty
- Can be generated via `tools/gen_ground_truth.py`
- Uses LLM judge to validate captured states

### Validation Scripts

#### 1. `evaluation/validate_queries.py`
Tests JMESPath queries directly against raw finish JSON (no expected_value comparison):

```bash
# Single task
uv run python multi-real/evaluation/validate_queries.py multi-real/tasks/dashdish-gomail-1.json \
  --finish-json multi-real/final_states/manual/dashdish-gomail-1.json

# All tasks with available finish JSONs
uv run python multi-real/evaluation/validate_queries.py --all \
  --finish-json-dir multi-real/final_states/manual
```

Returns:
- `passed`: Query executed and returned non-null
- `failed`: Query returned null or errored

#### 2. `evaluation/validate_ground_truth.py`
Tests JMESPath queries WITH expected_value comparison:

```bash
uv run python multi-real/evaluation/validate_ground_truth.py --verbose
uv run python multi-real/evaluation/validate_ground_truth.py --task gocalendar-gomail-omnizon-1
```

Returns:
- `pass`: Query result matches expected_value
- `fail`: Query result doesn't match expected_value
- `unverified`: expected_value contains "TODO"

### JMESPath Query Generation

Queries are generated by `tools/gen_tasks.py` using **Gemini 2.5 Flash**:

1. Reads `tasks.csv` (prompt, possible, websites columns)
2. For each task, calls Gemini with app-specific query patterns
3. Generates 2-4 evals per task with placeholder expected_values
4. Writes task JSON to `tasks/`

The prompt includes extensive patterns for each app type:
- `gomail`: `differences.emails.sent[0]`, `differences.emails.drafts[]`
- `gocalendar`: `differences.events.added.values(@)[0]`
- `opendining`: `differences.bookings.added[0]`
- `flyunified`: `differences.bookedFlights[0]`
- etc.

### Capture Pipeline

`capture_pipeline.py` runs agents to capture finish states:

```bash
# Single model
uv run python multi-real/capture_pipeline.py --model gpt-4o --max-steps 30 --task dashdish-gomail-1

# Multi-model with agreement checking
uv run python multi-real/capture_pipeline.py \
  --models claude-sonnet-3.7,gpt-4o,gemini-2.0 \
  --agreement-threshold 2 \
  --all
```

Outputs:
- Finish JSONs to `final_states/`
- Review CSV to `capture_review.csv`

### Hybrid Evaluation System

The `core/validator.py` implements a hybrid approach:

1. **Primary**: JMESPath queries (fast, deterministic)
2. **Fallback**: LLM judge when JMESPath fails but state changes exist

Confidence levels:
- `high`: JMESPath passed, or both agreed
- `medium`: LLM override (JMESPath failed, LLM passed)
- `low`: Both failed or no finish state

## Code References

- `multi-real/tasks/*.json` - Task definitions with evals
- `multi-real/core/registry.py:36` - MultiRealRegistry class loads tasks
- `multi-real/core/harness.py:171` - JMESPath evaluation logic
- `multi-real/core/validator.py:79` - HybridValidator.evaluate() method
- `multi-real/evaluation/validate_queries.py:48` - Query validation function
- `multi-real/evaluation/validate_ground_truth.py:58` - Ground truth validation
- `multi-real/tools/gen_tasks.py:307` - Gemini query generation prompt
- `multi-real/capture_pipeline.py:253` - Agent capture execution

## Architecture Documentation

### Task Flow
1. Tasks defined in `tasks.csv` (prompts, websites)
2. `gen_tasks.py` generates task JSONs with Gemini-created JMESPath evals
3. Tasks synced to package via `sync_tasks.py`
4. Agent runs via `run_benchmark.py` or `capture_pipeline.py`
5. Finish states captured from `/finish` endpoints
6. Validation via `validate_queries.py` or `validate_ground_truth.py`

### Evaluation Flow
1. Agent completes task, navigates to `/finish` on each website
2. Harness captures multi-app finish JSON
3. JMESPath queries run against finish state
4. If queries fail but state exists, LLM judge provides fallback

## Recommendations for Completing TODOs

Based on the codebase analysis, here are the approaches (ordered by preference):

### Option 1: Extend `validate_queries.py` (Recommended)
Add a `--fill-expected-values` flag that:
1. Runs each query against finish JSON
2. For boolean queries: sets expected_value to the result
3. For field access queries: sets expected_value to `null`
4. Updates the task JSON in place

**Pros**: Minimal new code, uses existing validation logic
**Cons**: Only works for tasks with ground truth

### Option 2: Extend `gen_tasks.py`
Modify the Gemini prompt to output proper expected_values instead of TODOs. Change line 445:
```python
"expected_value": <true/false for boolean queries, null for field access queries>
```
And remove the post-processing that sets TODOs.

**Pros**: Fixes at generation time
**Cons**: Requires regenerating all tasks

### Option 3: Create `tools/fill_expected_values.py`
New script that:
1. Loads all tasks with TODOs
2. For each, finds matching ground truth
3. Runs queries and fills expected_values
4. Falls back to query analysis (boolean vs field access) when no ground truth

**Pros**: Dedicated tool for this purpose
**Cons**: More new code to maintain

### Option 4: Use `gen_ground_truth.py` to expand coverage
Run more agent captures to get ground truth for more tasks, then use Option 1/3.

**Pros**: More comprehensive
**Cons**: Time-consuming, requires compute

## Current Benchmark Status (Post-Implementation)

### Scripts Created
1. **`tools/fill_expected_values.py`** - Fills expected_value fields from ground truth
2. **`tools/fix_query_patterns.py`** - Fixes schema mismatches in JMESPath queries

### Query Fixes Applied
- 115 queries fixed for GoMail schema (`emails.sent` → `emails.added[?sent == true]`)
- 10 queries fixed for UDriver schema (`pickup` → `pickup.address`)

### Validation Results (as of implementation)

```
validate_queries.py --all:
  52 passed, 42 failed (55% pass rate)
  19 tasks with broken queries

validate_ground_truth.py:
  12 passed (52.2%)
  5 failed
  4 errors
  2 unverified (TODO)
```

### Ground Truth Coverage
- **7 manual ground truth files** available
- **84 total tasks** in benchmark
- **8.3% coverage** with verified ground truth

### Issues Requiring Resolution for Production

1. **Ground Truth Coverage**: Only 7/84 tasks have ground truth (8.3%)
   - Use `capture_pipeline.py` to generate more
   - Prioritize high-value task combinations

2. **Query Errors**: 42 queries fail against ground truth
   - Main cause: Task matched to wrong ground truth file
   - Solution: Each task needs dedicated ground truth

3. **Schema Inconsistencies**: Some apps have unexpected structures
   - `topwork.differences.applications` - structure unknown
   - `zilloft.differences.favoritedListings` - may be null

4. **File Naming**: `gomail-marrsuite-1.json` vs `gomail-marrisuite-1.json` typo

### Recommended Path to Production-Ready Benchmark

**Phase 1: Expand Ground Truth (Critical)**
```bash
# Generate ground truth for remaining tasks
uv run python multi-real/capture_pipeline.py --model claude-sonnet-4 --all
```

**Phase 2: Fix Remaining Query Issues**
```bash
# Run validation to identify broken queries
uv run python multi-real/evaluation/validate_queries.py --all -v

# Fix each category of broken queries
uv run python multi-real/tools/fix_query_patterns.py
```

**Phase 3: Fill Expected Values**
```bash
# Fill from ground truth
uv run python multi-real/tools/fill_expected_values.py --verbose

# Review provenance report
cat multi-real/provenance_report.json | jq '.total_evals_flagged'
```

**Phase 4: Final Validation**
```bash
# Verify all queries work
uv run python multi-real/evaluation/validate_ground_truth.py --verbose

# Target: 90%+ pass rate for production
```

### Reliability Metrics for Publication

| Metric | Current | Target |
|--------|---------|--------|
| Ground truth coverage | 8.3% | 100% |
| Query pass rate | 55% | 95% |
| Expected value verification | 52% | 90% |
| Tasks fully validated | 1 (dashdish-gomail-1) | 84 |

### Provenance Report Location
`multi-real/provenance_report.json` - Full audit trail of expected_value derivations

## Open Questions

1. Should `expected_value: null` mean "query runs successfully" or "verify manually"?
2. How to handle tasks where the "correct" answer is context-dependent (e.g., date-based)?
3. Priority order for the 84 tasks - which ones need ground truth first?
4. Should the LLM judge be used to infer expected_values when no ground truth exists?
